Submitting job with sbatch from directory: /home/mohamed.aly/u11448/DeepLearning4NLP
Home directory: /user/mohamed.aly/u11448
Working directory: /home/mohamed.aly/u11448/DeepLearning4NLP
Current node: ggpu141
Python 3.10.14
Collecting environment information...
PyTorch version: 2.2.0
Is debug build: False
CUDA used to build PyTorch: 12.1
ROCM used to build PyTorch: N/A

OS: Rocky Linux 8.8 (Green Obsidian) (x86_64)
GCC version: (GCC) 8.5.0 20210514 (Red Hat 8.5.0-18)
Clang version: Could not collect
CMake version: version 3.27.4
Libc version: glibc-2.28

Python version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-4.18.0-477.21.1.el8_8.x86_64-x86_64-with-glibc2.28
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: 
GPU 0: NVIDIA A100-SXM4-80GB
GPU 1: NVIDIA A100-SXM4-80GB
GPU 2: NVIDIA A100-SXM4-80GB
GPU 3: NVIDIA A100-SXM4-80GB

Nvidia driver version: 535.104.12
cuDNN version: Probably one of the following:
/usr/lib64/libcudnn.so.8.9.5
/usr/lib64/libcudnn_adv_infer.so.8.9.5
/usr/lib64/libcudnn_adv_train.so.8.9.5
/usr/lib64/libcudnn_cnn_infer.so.8.9.5
/usr/lib64/libcudnn_cnn_train.so.8.9.5
/usr/lib64/libcudnn_ops_infer.so.8.9.5
/usr/lib64/libcudnn_ops_train.so.8.9.5
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:        x86_64
CPU op-mode(s):      32-bit, 64-bit
Byte Order:          Little Endian
CPU(s):              128
On-line CPU(s) list: 0-127
Thread(s) per core:  2
Core(s) per socket:  32
Socket(s):           2
NUMA node(s):        8
Vendor ID:           AuthenticAMD
CPU family:          25
Model:               1
Model name:          AMD EPYC 7513 32-Core Processor
Stepping:            1
CPU MHz:             2600.000
CPU max MHz:         3681.6399
CPU min MHz:         1500.0000
BogoMIPS:            5200.49
Virtualization:      AMD-V
L1d cache:           32K
L1i cache:           32K
L2 cache:            512K
L3 cache:            32768K
NUMA node0 CPU(s):   0-7,64-71
NUMA node1 CPU(s):   8-15,72-79
NUMA node2 CPU(s):   16-23,80-87
NUMA node3 CPU(s):   24-31,88-95
NUMA node4 CPU(s):   32-39,96-103
NUMA node5 CPU(s):   40-47,104-111
NUMA node6 CPU(s):   48-55,112-119
NUMA node7 CPU(s):   56-63,120-127
Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr wbnoinvd amd_ppin brs arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold v_vmsave_vmload vgif v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca fsrm

Versions of relevant libraries:
[pip3] numpy==1.26.4
[pip3] torch==2.2.0
[pip3] torch-tb-profiler==0.4.3
[pip3] torchaudio==2.2.0
[pip3] torchvision==0.17.0
[pip3] triton==2.2.0
[conda] blas                      1.0                         mkl  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] libjpeg-turbo             2.0.0                h9bf148f_0    pytorch
[conda] mkl                       2023.1.0         h213fc3f_46344  
[conda] mkl-fft                   1.3.8                    pypi_0    pypi
[conda] mkl-random                1.2.4                    pypi_0    pypi
[conda] mkl-service               2.4.0                    pypi_0    pypi
[conda] mkl_fft                   1.3.8           py310h5eee18b_0  
[conda] mkl_random                1.2.4           py310hdb19cb5_0  
[conda] numpy                     1.26.4                   pypi_0    pypi
[conda] numpy-base                1.26.4          py310hb5e798b_0  
[conda] pytorch                   2.2.0           py3.10_cuda12.1_cudnn8.9.2_0    pytorch
[conda] pytorch-cuda              12.1                 ha16c6d3_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torch                     2.2.0                    pypi_0    pypi
[conda] torch-tb-profiler         0.4.3                    pypi_0    pypi
[conda] torchaudio                2.2.0                    pypi_0    pypi
[conda] torchtriton               2.2.0                     py310    pytorch
[conda] torchvision               0.17.0                   pypi_0    pypi
[conda] triton                    2.2.0                    pypi_0    pypi

Current Branch: train_multitask_all_data-Aly
Latest Commit: 6a179df
Uncommitted Changes: 9

Loaded 7111 train examples from data/sst-sentiment-train.csv
Loaded 121620 train examples from data/quora-paraphrase-train.csv
Loaded 5149 train examples from data/sts-similarity-train.csv
Loaded 2019 train examples from data/etpc-paraphrase-train.csv
Loaded 2365 train examples from data/sst-sentiment-dev.csv
Loaded 40540 train examples from data/quora-paraphrase-dev.csv
Loaded 1709 train examples from data/sts-similarity-dev.csv
Loaded 505 train examples from data/etpc-paraphrase-dev.csv
config: namespace(hidden_dropout_prob=0.1, hidden_size=768, num_hidden_layers=12, data_dir='.', option='finetune', local_files_only=True, pooling='max', additional_inputs=False, add_layers=False, layers=[0], train_mode='all_pooled', dropout=True, max_position_embeddings=128)
------------------------------
    BERT Model Configuration
------------------------------
{'add_layers': False,
 'additional_inputs': False,
 'batch_size': 32,
 'dropout': True,
 'epochs': 5,
 'filepath': 'models1/finetune-5-2e-05-qqp.pt',
 'hidden_dropout_prob': 0.1,
 'improve_dir': './improve_dir',
 'layers': [0],
 'local_files_only': True,
 'lr': 2e-05,
 'optimizer': 'adamw',
 'option': 'finetune',
 'pooling': 'max',
 'qqp_improve_dir': './improve_dir/qqp',
 'scheduler': 'linear_warmup',
 'seed': 11711,
 'sst_improve_dir': './improve_dir/sst',
 'sts_improve_dir': './improve_dir/sts',
 'task': 'qqp',
 'train_mode': 'all_pooled',
 'use_gpu': True}
------------------------------
sts task not included in the training and evaluation
sst task not included in the training and evaluation
etpc task not included in the multitask training and evaluation
Paraphrase detection accuracy: 0.850


sts task not included in the training and evaluation
sst task not included in the training and evaluation
etpc task not included in the multitask training and evaluation
Paraphrase detection accuracy: 0.804


Epoch 01 (qqp): train loss :: 0.474, train :: 0.850, dev :: 0.804
Saving the model to models1/finetune-5-2e-05-qqp.pt.
sts task not included in the training and evaluation
sst task not included in the training and evaluation
etpc task not included in the multitask training and evaluation
Paraphrase detection accuracy: 0.902


sts task not included in the training and evaluation
sst task not included in the training and evaluation
etpc task not included in the multitask training and evaluation
Paraphrase detection accuracy: 0.814


Epoch 02 (qqp): train loss :: 0.352, train :: 0.902, dev :: 0.814
Saving the model to models1/finetune-5-2e-05-qqp.pt.
sts task not included in the training and evaluation
sst task not included in the training and evaluation
etpc task not included in the multitask training and evaluation
Paraphrase detection accuracy: 0.938


sts task not included in the training and evaluation
sst task not included in the training and evaluation
etpc task not included in the multitask training and evaluation
Paraphrase detection accuracy: 0.812


Epoch 03 (qqp): train loss :: 0.258, train :: 0.938, dev :: 0.812
sts task not included in the training and evaluation
sst task not included in the training and evaluation
etpc task not included in the multitask training and evaluation
Paraphrase detection accuracy: 0.967


sts task not included in the training and evaluation
sst task not included in the training and evaluation
etpc task not included in the multitask training and evaluation
Paraphrase detection accuracy: 0.819


Epoch 04 (qqp): train loss :: 0.180, train :: 0.967, dev :: 0.819
Saving the model to models1/finetune-5-2e-05-qqp.pt.
sts task not included in the training and evaluation
sst task not included in the training and evaluation
etpc task not included in the multitask training and evaluation
Paraphrase detection accuracy: 0.974


sts task not included in the training and evaluation
sst task not included in the training and evaluation
etpc task not included in the multitask training and evaluation
Paraphrase detection accuracy: 0.807


Epoch 05 (qqp): train loss :: 0.127, train :: 0.974, dev :: 0.807
Loaded model to test from models1/finetune-5-2e-05-qqp.pt
Loaded 2371 test examples from data/sst-sentiment-test-student.csv
Loaded 40540 test examples from data/quora-paraphrase-test-student.csv
Loaded 1721 test examples from data/sts-similarity-test-student.csv
Loaded 574 test examples from data/etpc-paraphrase-detection-test-student.csv
Loaded 2365 dev examples from data/sst-sentiment-dev.csv
Loaded 40540 dev examples from data/quora-paraphrase-dev.csv
Loaded 1709 dev examples from data/sts-similarity-dev.csv
Loaded 505 dev examples from data/etpc-paraphrase-dev.csv
sts task not included in the training and evaluation
sst task not included in the training and evaluation
etpc task not included in the multitask training and evaluation
Paraphrase detection accuracy: 0.819


etpc task not included in the multitask training and evaluation
dev paraphrase acc :: 0.819
Your score is higher than the baseline for qqp task
saving params
